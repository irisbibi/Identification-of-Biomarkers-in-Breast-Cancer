# Identification of Biomarkers in Breast Cancer

In this project, the goal is to identify biomarkers of breast cancer using RNA-seq data comprising samples of normal and diseased groups without treatments from The Cancer Genome Atlas (TCGA) using different machine learning approaches.

The first two methods used are supervised learning, logistic lasso regression and random forest to build classification models based on the different level of gene expression between normal and diseased groups, and identify the most informative genes as biomarker genes. The third method used is an unsupervised method to identify co-expressed gene clusters and find out the genes of high connectivity within each cluster. After identifying the marker genes using three methods, the results generated by these methods were compared and annotated with Gene Ontology enrichment analysis to distinguish the enriched biological pathways.

The data set used was a high-throughput sequence data downloaded from TCGA that contains rna-seq counts with 60,483 genes and 797 samples. The samples contain two classes, 708 cancer and 89 normal patients without any drug treatment.
## Method
1 Data Preprocessing and Identification of DEGs

To account for sequencing depth and RNA composition, we used trimmed mean of M values (TMM) normalization method to normalize the raw counts [15]. Then we use the
normalized counts to filter out the low-expressed genes by only keeping the genes with a positive read counts in at least20%of the samples. Finally, we added 1 to each data point and performed a log base 2 transformation to ensure that all values fall in a relative small range and remain non-negative.

To identify differentially expressed genes between BC and normal groups, T-test and Benjamini and Hochberg (false discovery rate) were used to calculate the p-value and FDR [7]. A criterion of |log 2 (fold change)| > 2 and p < 0.001 was used to filter DEGs[4].

2 Supervised Learning

2.1 Logistic Lasso Regression

Since not all genes will contribute to BC, lasso regularization has the advantage of providing feature selection strategy by forcing certain coefficients to zero [14]. Hence, we combined it with logistic regression to build a probabilistic model.

To build the logistic lasso regression model, we applied log loss function with L penalty item to calculate the loss and used gradient descent method to update the coefficients in each epoch until we get the optimal coefficients [10]. This can ensure that most of the coefficients is 0.

We used the five-fold cross validation to validate the logistic lasso regression model results [5]. Every time we used80%of the data set as training set, and20%of the data set as testing set. In the first iteration, we used the first fold data to test the model, and the rest of the data to train the model and then continued the training until each of the five folds has been utilized as a test set.

The higher the coefficient of a gene, the higher the chance it will contribute to the BC. The top 50 genes with the highest coefficients were selected as biomarkers.

2.2 Random Forest

We chose random forest because of its robustness against overfitting, the direct and easy interpretation of the model and its excellent performance on high dimensional data [1]. Besides, the Gini importance of the random forest can serve as a method of feature selection to help identify key features [19].

Random forests are bagged decision tree models [9]. We can select parameters randomly without the need to decided every parameter and we can also choose the best split in all the random splits instead of just choosing among the potential thresholds of each node [12]. For the implemented random forest model, it only considers a small subset of features and samples randomly to build each tree (bootstrapping), and classifies a sample as a majority vote of all decision trees in the forest. In our project, the final parameters are chosen as: bootstrap_size= 0. 632 , max_depth = 1000 , min_sample_size = 2 , random_features= 800 , random_splits= 800 , forest_size= 100.

In order to evaluate our model, the out-of-bag (OOB) score is calculated for each built tree [1]. The OOB samples are the samples that were not selected to build a particular tree. Once a tree is built with the bootstrapped samples, the OOB score can be computed as the mean prediction accuracy from those OOB samples, and the final OOB score for the entire forest is the average of all those scores. This is an estimation for how accurate the random forest performs, which is essentially the leave-one-out cross validation [3].

Gini importance (Gj) of the node (j) is calculated as the decrease in node impurity (Cj) weighted by the probability of reaching that node (wj). The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The final Gini importance for each feature is divided by the total number of times it appears in all decision trees (nj). The higher the value, the more important the feature 

2.3 Unsupervised Learning

Here we use clustering to simulate gene regulatory network, which is composed of co-expressed gene
modules [23]. We expected a strong correlation of expression between genes within a module but not
between modules, which can be constructed using clustering methods. The marker genes are selected
as the genes with the highest connectivity with other genes in the cluster, which can be interpreted as
key regulators of other genes.

2.3.1 Hierarchical Clustering: Agglomerative clustering

Hierarchical clustering determines the clusters by constructing a hierarchy. The bottom-up agglomerative clustering is used here, which merges the 2 most similar data points until all points have been merged into a single cluster. Average-linkage is used as the distance metric in this project[17].

Since we do not have a ground truth label here, we will evaluate the clustering performance using the following two methods:

1. Elbow method
    Finding the clustering step where the acceleration of the distance growth is the most significant step in this method. This is the same as finding the step with the second highest
    derivative of the distances [23].
2. Inconsistency method
    Compare the heighthof each cluster merge to the average and then normalize it by the standard deviation overdprevious levels, wheredis the selected depth. We use inconsistency
    function in scipy.cluster.hierarchy to implement this method. We also use cluster function in scipy.cluster.hierarchy to retrieve the clusters [18].

2.3.2 Partitional Clustering: K-means clustering

Partitional clustering divides data points intoknon-overlapping groups. The K-means algorithm is used here, which randomly initializeskcentroids and implements EM algorithm to
assign each data point to the final cluster [7].

Since we do not have a ground truth label here, we will evaluate the clustering performance using the following 2 methods:

1. Elbow method
    The elbow method evaluates what would be a goodkbased one the sum of squared distance (SSE) between each data point and its assigned centroid. We picked the elbow point to be where the SSE curve starts to bend. We used elbow function in kneed to determine the elbow point, thus choosingk[12].
2. Silhouette coefficient value
    Silhouette coefficient determines the degree of separation between clusters, which takes value betweenâˆ’ 1 and 1. A larger coefficient value indicates a higher similarity within
    clusters. We used silhouette-score function in sklearn.cluster to calculate the silhouette coefficient [18].

Each cluster can be interpreted as a module in gene-regulatory network. Pearson correlation is used to calculate the pairwise similarity between each pair of genes. And the
connectivity of a gene is defined as the sum of its interaction with all other genes in a module. We finally output genes within top10%connectivity for each module> 50 genes.

## Results

3.1 Data Preprocessing and Identification of DEGs

After filtering the low-expressed the genes, the number of features decreased to 19369, which was a more than 30 % decrease (Figure 1). However, there are more significant genes expressed in the genes than we expected, which may indicate some errors in our data. We identified in total 808 DEGs.

3.2 Logistic Lasso Regression

We used Five-Fold Cross Validation to evaluate the logistic lasso regression model. In each fold, we randomly split the data into 80%of training set and 20%of testing set to calculate Receiver Operating Characteristic (ROC).

The mean AUC is 0. 99 > 0. 95 , which shows the great performance of the data. And we chose the first fold validation result that has the same AUC as the mean AUC to make a confusion matrix to further analyze the average performance of the model. 

Based on the optimal coefficients, we ranked the coefficients and extracted genes with top 50 highest coefficients, including: SDS, LAMP5, F12, BMPR1B, ASPM, GJB2, KIF18B, HJURP, etc.

3.3 Random Forest

We randomly split the data into training and testing subsets. When the parameters are chosen as mentioned above, The train accuracy is 1.0, the test accuracy is 0.9958 and the OOB score (validation accuracy) is 0.9697. The implemented random forest model has an excellent performance for classifying the normal samples and BC tumor samples. With the number of trees in the forest increased, the train, validation and test accuracy for the model also increased.

The marker genes identified by the random forest model are the features with top 50 highest feature importance, including: LINC01614, NUF2, WISP1, PCLAF, BICDL1, IQGAP3, CILP2, PLPP4, MMP13, COL11A1, PKMYT1, MMP11, NEK2, COL10A1 and so on.

3.4 Clustering

Agglomerative Clustering 
The elbow method suggests that the optimal cluster number should be 4. However, a closer look at the dendrogram of gene clustering results suggests that agglomerative clustering may not be suitable for this dataset. Wherever we apply a threshold to the dendrogram, we can observe the existence of outlier clusters, which are characterized by relative small gene numbers and separated expression pattern. There are still many subclusters within a large cluster that cannot be detected. A possible reason of this would be the outliers in the normalized data. All of the outlier data points have al og 2 (read counts)âˆˆ(6,15), which indicate the raw read countsâˆˆ(2^6 , 215 ). The outliers with a relatively high expression may have a strong effect on data analysis.

The clustering results are obtained by inconsistency method, which utilizes the data after continuously removing the obvious outlier clusters, validated our assumption. There are still outlier clusters and sub-clusters within a large cluster even if we continuously remove the outlier clusters.

K-means Clustering 
Since there are many outliers using agglomerative clustering, we turned to K-means clustering since it is more robust to outliers.

Taking both SSE and silhouette scores into account, we selectedK= 7as the optimal cluster number.. We also performed PCA and reduced the data onto 2 dimensions with the greatest variance, then plotted the clustering results based on the reduced data. There were very obvious cluster patterns on the reduced data and the outlier data points form a cluster themselves (purple one, relatively far away from other data points), which can be removed for the downstream analysis. This also validated the assumption that K-means clustering method is more robust for outliers.

After removing the outlier cluster, which has 14 genes, there were 6 clusters remaining with 57 , 349 , 116 , 102 , 111 , 59 genes. For each cluster, we identified biomarkers with top10%connectivity, which lead to a total of 69 biomarkers, including: ZMYND10, DEPDC1B, EPN3, POLQ, COL11A1, PAFAH1B3, RAD54L, KIF4A, ORC6., etc.

3.5 Gene Ontology

We defined the marker genes as genes identified at least twice in all three methods, and there are in total 20 marker genes from three methods. Gene ontology enrichment analysis results indicated that the identified biomarkers are related with tumor progression and metastasis. Specifically, three genes identified in all three methods are existing biomarkers for BC. For example, MMP11 is a matrix metalloproteinase (MMP) enzyme and studies have shown that MMP11 knock-down could inhibit tumour proliferation and growth [29]. Collagen type X alpha 1 (COL10A1) is overexpressed in breast cancer and displays vital roles in tumorigenesis, which is up-regulated in different subtypes of breast cancer, positively associated with progesterone receptor, human epidermal growth factor receptor-2 status and nodal status [27].